{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc. \n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Class\n",
    "\n",
    "In this section, we will use the previous code to build a dataset class. As before, make sure the even sampes are positve and the odd samples are negative. If parameter `train` is set to **True** , use the first 30000 samples as training data; otherwise, the remaining samples will be used as validation data. Do not forget to sort you files so they are in the same order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, transform=None , train=True):\n",
    "        \n",
    "        directory=\"./resources/data\"\n",
    "        positive=\"Positive\"\n",
    "        negative=\"Negative\"\n",
    "        \n",
    "        positive_file_path=os.path.join(directory , positive)\n",
    "        negative_file_path=os.path.join(directory , negative)\n",
    "        \n",
    "        positive_files=[os.path.join(positive_file_path , file) for file in os.listdir(positive_file_path) if file.endswith(\".jpg\")]\n",
    "        positive_files.sort()\n",
    "        \n",
    "        negative_files=[os.path.join(negative_file_path , file) for file in os.listdir(negative_file_path) if file.endswith(\".jpg\")]\n",
    "        negative_files.sort()\n",
    "        \n",
    "        number_of_samples = len(positive_files) + len(negative_files)\n",
    "        \n",
    "        self.all_files = [None] * number_of_samples\n",
    "        self.all_files[::2] = positive_files\n",
    "        self.all_files[1::2] = negative_files\n",
    "        \n",
    "        self.transform=transform\n",
    "        \n",
    "        self.Y=torch.zeros([number_of_samples]).type(torch.LongTensor)\n",
    "        self.Y[::2] = 1\n",
    "        self.Y[1::2] = 0\n",
    "        \n",
    "        if train:\n",
    "            self.all_files = self.all_files[:30000]\n",
    "            self.Y = self.Y[:30000]\n",
    "            self.len = len(self.all_files)\n",
    "        else:\n",
    "            self.all_files = self.all_files[30000:]\n",
    "            self.Y = self.Y[30000:]\n",
    "            self.len = len(self.all_files)\n",
    "    \n",
    "    def __getitem__ (self, idx):\n",
    "        \n",
    "        image=Image.open(self.all_files[idx])\n",
    "        y = self.Y[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    # note to self, in pytorch the target has to be a single number from the inteval [0,#classes]\n",
    "    #instead of being one hot encoded vector\n",
    "    def vectorize(y, number_of_classes):\n",
    "        out = torch.zeros(number_of_classes).type(torch.IntTensor)    \n",
    "        out[y] = 1\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Object and Dataset Object\n",
    "\n",
    "Create a transform object, that uses the `Compose` function. First use the transform `ToTensor()` and followed by `Normalize(mean, std)`. The value for **mean** and **std** are provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean, std)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an object for the training data `dataset_train` and validation `dataset_val`. Use the transform object to convert the images to tensors using the transform object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train=Dataset(transform=transform, train=True)\n",
    "dataset_val=Dataset(transform=transform, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the shape of the image :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 227, 227])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154587"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_of_image = 3*227*227\n",
    "size_of_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a custom module for Softmax for two classes, called model. The input size should be the `size_of_image`, you should record the maximum accuracy achieved on the validation data for the different epochs. For example, if after 5 epochs the accuracy was *0.5,0.2,0.64,0.77,0.66* you would select 0.77.\n",
    "\n",
    "Train the model with the following free parameter values:\n",
    "\n",
    "**Parameter Values**\n",
    "\n",
    "* learning rate:0.1\n",
    "* momentum term:0.1\n",
    "* batch size training : 1000\n",
    "* loss function: Cross Entropy Loss\n",
    "* epochs: 5\n",
    "* set torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x29e14b75430>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMax(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(SoftMax, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftMax(size_of_image, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001 , momentum=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criterion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loader Training and Validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=dataset_train, batch_size=1000)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=dataset_val, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model with 5 epochs, should take 35 minutes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs, model, optimizer, costFunc , training_loader, validation_loader, num_of_test ):\n",
    "    \n",
    "    accuracy_list = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for X, y in training_loader:\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            yhat = model(X.view(-1, 3 * 227 * 227))  \n",
    "                        \n",
    "            loss = costFunc(yhat , y)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "        \n",
    "        correct = 0    \n",
    "        for X_val , y_val in validation_loader:\n",
    "   \n",
    "            yhat_val = model(X_val.view(-1, 3 * 227 * 227))\n",
    "                        \n",
    "            _ , prediction = torch.max(yhat_val, 1)\n",
    "            correct += (prediction == y_val).sum().item()\n",
    "        \n",
    "        accuracy = correct / num_of_test\n",
    "        print(accuracy)\n",
    "        accuracy_list.append(accuracy)\n",
    "        \n",
    "    return accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7237\n",
      "0.7297\n",
      "0.7319\n",
      "0.7357\n",
      "0.7391\n",
      "Wall time: 7min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "accuracies = train_model(5, model, optimizer, cost, train_loader, validation_loader, len(dataset_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7237, 0.7297, 0.7319, 0.7357, 0.7391]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "print(dataset_val[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = SoftMax(size_of_image, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = model(dataset_train[4][0].view(-1, 3* 227*227))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4296, 0.5704]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[4][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = model2(dataset_train[8][0].view(-1, 3* 227*227))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6491, 0.3509]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "val , predict = torch.max(t.data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6491])"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1083, 0.1254, 0.1768,  ..., 0.2453, 0.2453, 0.2453],\n",
       "         [0.1426, 0.1597, 0.1768,  ..., 0.2453, 0.2453, 0.2453],\n",
       "         [0.2111, 0.1939, 0.1939,  ..., 0.2453, 0.2453, 0.2453],\n",
       "         ...,\n",
       "         [0.2282, 0.2453, 0.2453,  ..., 0.2624, 0.2624, 0.2624],\n",
       "         [0.2282, 0.2453, 0.2453,  ..., 0.2624, 0.2624, 0.2624],\n",
       "         [0.2282, 0.2453, 0.2453,  ..., 0.2624, 0.2624, 0.2624]],\n",
       "\n",
       "        [[0.3102, 0.3277, 0.3803,  ..., 0.4153, 0.4153, 0.4153],\n",
       "         [0.3452, 0.3627, 0.3803,  ..., 0.4153, 0.4153, 0.4153],\n",
       "         [0.4153, 0.3978, 0.3978,  ..., 0.4153, 0.4153, 0.4153],\n",
       "         ...,\n",
       "         [0.3803, 0.3978, 0.3978,  ..., 0.4328, 0.4328, 0.4328],\n",
       "         [0.3803, 0.3978, 0.3978,  ..., 0.4328, 0.4328, 0.4328],\n",
       "         [0.3803, 0.3978, 0.3978,  ..., 0.4328, 0.4328, 0.4328]],\n",
       "\n",
       "        [[0.5485, 0.5659, 0.6182,  ..., 0.6182, 0.6182, 0.6182],\n",
       "         [0.5834, 0.6008, 0.6182,  ..., 0.6182, 0.6182, 0.6182],\n",
       "         [0.6531, 0.6356, 0.6356,  ..., 0.6182, 0.6182, 0.6182],\n",
       "         ...,\n",
       "         [0.6356, 0.6531, 0.6531,  ..., 0.6356, 0.6356, 0.6356],\n",
       "         [0.6356, 0.6531, 0.6531,  ..., 0.6356, 0.6356, 0.6356],\n",
       "         [0.6356, 0.6531, 0.6531,  ..., 0.6356, 0.6356, 0.6356]]])"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[4][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0054,  0.0069,  0.0036,  ...,  0.0045,  0.0050,  0.0060],\n",
       "        [-0.0048, -0.0037, -0.0066,  ..., -0.0071, -0.0071, -0.0070]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual Softmax Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.tensor([[1.,3.,4.,5.,6.,7.,8.], [6.,4.,6.,3.,7.,2.,1.], [3.,1.,1.,1.,1.,2.,1.], [3.5,7.,7.,5.,8.,2.,8.,]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 7])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Linear(7 , 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6249, -3.7367,  2.5930],\n",
       "        [ 0.1869, -2.9774,  1.7395],\n",
       "        [-1.2051, -0.7701,  1.1830],\n",
       "        [-0.2920, -4.3519,  2.0014]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = linear(h)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0145, 0.0018, 0.9838],\n",
       "        [0.1734, 0.0073, 0.8192],\n",
       "        [0.0744, 0.1150, 0.8106],\n",
       "        [0.0915, 0.0016, 0.9069]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = softmax(z)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0145, 0.0018, 0.9838],\n",
       "        [0.1734, 0.0073, 0.8192],\n",
       "        [0.0744, 0.1150, 0.8106],\n",
       "        [0.0915, 0.0016, 0.9069]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ , yhat = torch.max(s, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 2, 2])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1], dtype=torch.int32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones([4]).type(torch.IntTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 0], dtype=torch.int32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vectorize(y, number_of_classes):\n",
    "    out = torch.zeros(number_of_classes).type(torch.IntTensor)    \n",
    "    out[y] = 1\n",
    "    return out\n",
    "\n",
    "\n",
    "vectorize(0 , 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
